{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import urllib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "# Global directory variables\n",
    "decompressed_dir = None\n",
    "compressed_dir = None\n",
    "subfolder_path = None\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='error_log.txt', level=logging.ERROR,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Custom function to log errors\n",
    "def log_error(message):\n",
    "    print(message)\n",
    "    logging.error(message)\n",
    "\n",
    "# Custom function to log general messages\n",
    "def log_message(message):\n",
    "    print(message)\n",
    "\n",
    "download_delay = 0.05\n",
    "last_downloaded_time = time.time()\n",
    "\n",
    "def download_file(url, file_path):\n",
    "    global last_downloaded_time\n",
    "    if os.path.exists(file_path):\n",
    "        log_message(f\"File {file_path} already exists. Skipping download.\")\n",
    "        return file_path\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        time.sleep(download_delay)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "\n",
    "        if file_size < 150*1024:\n",
    "            os.remove(file_path)\n",
    "            log_error(f\"Deleted {file_path} as its size was less than 1KB.\")\n",
    "            return None\n",
    "\n",
    "        last_downloaded_time = time.time()\n",
    "        return file_path\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        log_error(f\"Failed to download {url}. Error: {e}\")\n",
    "    return None\n",
    "\n",
    "def monitor_downloads(timeout=3):\n",
    "    global last_downloaded_time\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        if time.time() - last_downloaded_time > timeout:\n",
    "            log_message(\"Monitoring timeout. Stopping the download process.\")\n",
    "            return False\n",
    "\n",
    "def get_xml(date, path, max_concurrent_downloads):\n",
    "    global last_downloaded_time\n",
    "\n",
    "    try:\n",
    "        input_date = datetime.strptime(date, '%Y%m%d')\n",
    "        if input_date.date() >= datetime.now().date():\n",
    "            log_error(f\"{date} is not in the past.\")\n",
    "            return\n",
    "    except ValueError:\n",
    "        log_error(f\"{date} is not a valid date in YYYYMMDD format.\")\n",
    "        return\n",
    "\n",
    "    subfolder_path = os.path.join(path, date)\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.mkdir(subfolder_path)\n",
    "        log_message(f\"Created subfolder at {subfolder_path}\")\n",
    "    else:\n",
    "        log_message(f\"Subfolder {subfolder_path} already exists.\")\n",
    "\n",
    "    download_attempts = 0\n",
    "    downloaded_files = []\n",
    "    while download_attempts < 2:\n",
    "        log_message(f\"Download attempt {download_attempts + 1}\")\n",
    "\n",
    "        last_downloaded_time = time.time()\n",
    "        downloaded_files = []\n",
    "        with ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor:\n",
    "            futures = []\n",
    "            for current_time in [f\"{hour:02d}{minute:02d}\" for hour in range(24) for minute in range(60)]:\n",
    "                filename = f'VDLive_{current_time}.xml.gz'\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "                if os.path.isfile(file_path) and os.path.getsize(file_path) > 1024:\n",
    "                    downloaded_files.append(filename)\n",
    "                    continue\n",
    "\n",
    "                url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n",
    "                futures.append(executor.submit(download_file, url, file_path))\n",
    "\n",
    "            monitor_thread = threading.Thread(target=monitor_downloads)\n",
    "            monitor_thread.start()\n",
    "\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    downloaded_files.append(os.path.basename(result))\n",
    "\n",
    "            monitor_thread.join()\n",
    "\n",
    "            download_attempts += 1\n",
    "            if len(downloaded_files) == 1440:\n",
    "                break\n",
    "            elif len(downloaded_files) > 1400:\n",
    "                max_concurrent_downloads = 1\n",
    "\n",
    "    log_message(f\"Downloaded {len(downloaded_files)} out of 1440 files.\")\n",
    "\n",
    "    if len(downloaded_files) != 1440:\n",
    "        all_files = [f'VDLive_{hour:02d}{minute:02d}.xml.gz' for hour in range(24) for minute in range(60)]\n",
    "        missing_files = [f for f in all_files if not os.path.isfile(os.path.join(subfolder_path, f))]\n",
    "    \n",
    "        for missing_file in missing_files:\n",
    "            log_error(f\"Failed to download or found corrupted: {missing_file}\")\n",
    "\n",
    "    return downloaded_files, subfolder_path\n",
    "\n",
    "def decompress_xml(downloaded_files, subfolder_path, date):\n",
    "    decompressed_dir = os.path.join(subfolder_path, f\"decompressed_{date}\")\n",
    "    compressed_dir = os.path.join(subfolder_path, f\"compressed_{date}\")\n",
    "\n",
    "    if not os.path.exists(decompressed_dir):\n",
    "        os.mkdir(decompressed_dir)\n",
    "        log_message(f\"Created decompressed directory at {decompressed_dir}\")\n",
    "    else:\n",
    "        log_message(f\"Decompressed directory {decompressed_dir} already exists.\")\n",
    "\n",
    "    if not os.path.exists(compressed_dir):\n",
    "        os.mkdir(compressed_dir)\n",
    "        log_message(f\"Created compressed directory at {compressed_dir}\")\n",
    "    else:\n",
    "        log_message(f\"Compressed directory {compressed_dir} already exists.\")\n",
    "\n",
    "    log_message(\"Starting decompression...\")\n",
    "\n",
    "    for file in tqdm(downloaded_files):\n",
    "        new_xml_filename = file.replace('.gz', '')\n",
    "        decompressed_file_path = os.path.join(decompressed_dir, new_xml_filename)\n",
    "\n",
    "        if os.path.exists(decompressed_file_path):\n",
    "            log_message(f\"File {decompressed_file_path} already decompressed. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with gzip.open(os.path.join(subfolder_path, file), 'rb') as f_in:\n",
    "            xml_data = f_in.read()\n",
    "            with open(decompressed_file_path, 'wb') as f_out:\n",
    "                f_out.write(xml_data)\n",
    "\n",
    "        os.rename(os.path.join(subfolder_path, file), os.path.join(compressed_dir, file))\n",
    "\n",
    "    log_message(\"Decompression complete. Generating error report...\")\n",
    "\n",
    "    all_files = [f'VDLive_{hour:02d}{minute:02d}.xml.gz' for hour in range(24) for minute in range(60)]\n",
    "    missing_compressed = [f for f in all_files if not os.path.exists(os.path.join(compressed_dir, f))]\n",
    "    missing_decompressed = [f.replace('.gz', '') for f in missing_compressed]\n",
    "\n",
    "    report_file_path = os.path.join(subfolder_path, f\"{date}_error_report.txt\")\n",
    "    with open(report_file_path, 'w') as f:\n",
    "        f.write(\"Missing Compressed Files:\\n\")\n",
    "        for file in missing_compressed:\n",
    "            f.write(f\"{file}\\n\")\n",
    "        f.write(\"\\nMissing Decompressed Files:\\n\")\n",
    "        for file in missing_decompressed:\n",
    "            f.write(f\"{file}\\n\")\n",
    "\n",
    "    log_message(f\"Error report saved to {report_file_path}\")\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "   \n",
    "\n",
    "def get_vd(date, time, desired_vdids, dataset):\n",
    "    results = {v: None for v in desired_vdids}\n",
    "    \n",
    "    directory = os.path.join(dataset, date, f\"decompressed_{date}\", f\"VDLive_{time}.xml\")\n",
    "    \n",
    "    if not os.path.isfile(directory):\n",
    "        for vdid in desired_vdids:\n",
    "            results[vdid] = {\"error\": f\"File {directory} not found.\"}\n",
    "        return results\n",
    "    elif os.path.getsize(directory) == 0:\n",
    "        for vdid in desired_vdids:\n",
    "            results[vdid] = {\"error\": f\"File {directory} is empty.\"}\n",
    "        return results\n",
    "\n",
    "    with open(directory) as xml_file:\n",
    "        data_dict = xmltodict.parse(xml_file.read())\n",
    "    json_data = json.dumps(data_dict)\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "    for vdlive in data[\"VDLiveList\"][\"VDLives\"][\"VDLive\"]:\n",
    "        if vdlive[\"VDID\"] in desired_vdids:\n",
    "            desired_vdids.remove(vdlive[\"VDID\"])  # This VDID is found; no need to search for it again\n",
    "            \n",
    "            link_flows = vdlive[\"LinkFlows\"][\"LinkFlow\"]\n",
    "            date_time = f'{date}_{time}'\n",
    "\n",
    "            vdid_data = {\n",
    "                \"VDID\": vdlive[\"VDID\"],\n",
    "                \"LinkFlows\": link_flows,\n",
    "                \"Status\": vdlive[\"Status\"],\n",
    "                \"DateTime\": date_time\n",
    "            }\n",
    "            results[vdlive[\"VDID\"]] = process_data(vdid_data)\n",
    "\n",
    "            # If we've found all the desired VDIDs, we can exit early\n",
    "            if not desired_vdids:\n",
    "                break\n",
    "                \n",
    "    for vdid in desired_vdids:  # VDIDs not found\n",
    "        results[vdid] = {\"absent\": str(time)}\n",
    "                \n",
    "    return results\n",
    "\n",
    "# [Rest of the code remains unchanged]\n",
    "def process_data(data):\n",
    "    rows = []\n",
    "    vdid = data['VDID']\n",
    "    date_time = data['DateTime']\n",
    "    link_id = data['LinkFlows']['LinkID']\n",
    "\n",
    "    # Situation 1: 'Lane' key is present in the 'Lanes' dictionary\n",
    "    if 'Lane' in data['LinkFlows']['Lanes']:\n",
    "        lanes = data['LinkFlows']['Lanes']['Lane']\n",
    "        if isinstance(lanes, dict):\n",
    "            lanes = [lanes]\n",
    "        for lane in lanes:\n",
    "            rows.extend(process_lane(lane, date_time, vdid, link_id))\n",
    "\n",
    "    # Situation 2: 'Lane' key is not present in the 'Lanes' dictionary\n",
    "    elif 'Lane' in data['LinkFlows']['Lanes']:\n",
    "        lane = data['LinkFlows']['Lanes']['Lane']\n",
    "        rows.extend(process_lane(lane, date_time, vdid, link_id))\n",
    "\n",
    "    return rows\n",
    "\n",
    "def process_lane(lane, date_time, vdid, link_id):\n",
    "    rows = []\n",
    "    lane_id = lane['LaneID']\n",
    "    lane_type = lane['LaneType']\n",
    "    lane_speed = lane['Speed']\n",
    "    occupancy = lane['Occupancy']\n",
    "    for vehicle in lane['Vehicles']['Vehicle']:\n",
    "        vehicle_type = vehicle['VehicleType']\n",
    "        volume = vehicle['Volume']\n",
    "        speed = vehicle['Speed']\n",
    "        rows.append([date_time, vdid, link_id, lane_id,\n",
    "                     lane_type, lane_speed, occupancy,\n",
    "                     vehicle_type, volume, speed])\n",
    "    return rows\n",
    "\n",
    "def get_vds(date, desired_vdids, dataset):\n",
    "    all_results = {vdid: [] for vdid in desired_vdids}\n",
    "    error_logs = {vdid: [] for vdid in desired_vdids}\n",
    "\n",
    "    for current_time in tqdm([f\"{hour:02d}{minute:02d}\" for hour in range(1) for minute in range(60)]):\n",
    "        results = get_vd(date, current_time, desired_vdids.copy(), dataset)\n",
    "        for vdid, result in results.items():\n",
    "            if \"error\" in result:\n",
    "                error_logs[vdid].append(result[\"error\"])\n",
    "            elif \"absent\" in result:\n",
    "                error_logs[vdid].append(f\"VDID absent at time: {result['absent']}\")\n",
    "            else:\n",
    "                all_results[vdid].extend(result)\n",
    "\n",
    "    for vdid in desired_vdids:\n",
    "        if all_results[vdid]:\n",
    "            # Process and save the results\n",
    "            df = pd.DataFrame(all_results[vdid], columns=['DateTime', 'VDID', 'LinkID', 'LaneID', 'LaneType', 'LaneSpeed', 'Occupancy', 'VehicleType', 'Volume', 'Speed'])\n",
    "            \n",
    "            #pivot dataframe according to vehicle\n",
    "            df = df.pivot(index=['DateTime', 'VDID', 'LinkID', 'LaneID', 'LaneType', 'LaneSpeed', 'Occupancy'], columns='VehicleType', values=['Volume', 'Speed'])\n",
    "            df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
    "            df = df.reset_index()\n",
    "            df = df.reindex(columns=['DateTime', 'VDID', 'LinkID', 'LaneID','LaneType', 'LaneSpeed', 'Occupancy', 'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T'])\n",
    "            \n",
    "            #pivot the dataframe according to lane\n",
    "            df = df.pivot_table(index=['DateTime', 'VDID'], columns='LaneID', values=['LaneSpeed', 'Occupancy', 'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T']).reset_index()\n",
    "            #flatten the column names\n",
    "            df.columns = ['_'.join(str(col).strip() for col in tup) for tup in df.columns.values]\n",
    "\n",
    "            filename = os.path.join(dataset, date, f\"{vdid}_{date}\")\n",
    "            df.to_csv(f'{filename}.csv', index=False)\n",
    "            print(f\"{filename}.csv saved successfully!\")\n",
    "        \n",
    "        # Save the error logs\n",
    "        if error_logs[vdid]:\n",
    "            filename = os.path.join(dataset, date, f\"{vdid}_{date}_errorlog.txt\")\n",
    "            with open(filename, 'w') as log_file:\n",
    "                log_file.write(\"\\n\".join(error_logs[vdid]))\n",
    "            print(f\"{filename} saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created subfolder at D:\\VD_data\\20230825\n",
      "Download attempt 1\n",
      "Monitoring timeout. Stopping the download process.\n",
      "Downloaded 1440 out of 1440 files.\n",
      "Created decompressed directory at D:\\VD_data\\20230825\\decompressed_20230825\n",
      "Created compressed directory at D:\\VD_data\\20230825\\compressed_20230825\n",
      "Starting decompression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [00:48<00:00, 29.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompression complete. Generating error report...\n",
      "Error report saved to D:\\VD_data\\20230825\\20230825_error_report.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    date = \"20230825\"\n",
    "    path = r\"D:\\VD_data\"\n",
    "    max_concurrent_downloads = 20\n",
    "\n",
    "    downloaded_files, subfolder_path = get_xml(date, path, max_concurrent_downloads)\n",
    "    decompress_xml(downloaded_files, subfolder_path, date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:55<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\VD_data\\20230825\\VD-N1-N-198-O-SE-21-北上出口-彰化_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-N1-S-369.007-M-Loop_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-N1-S-369.400-M-Loop_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-N1-S-370.000-M-Loop_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-N1-S-371.010-M-Loop_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-T82-W-14-O-EN-1-Loop_20230825.csv saved successfully!\n",
      "D:\\VD_data\\20230825\\VD-N1-N-33-I-EN-31-五股_20230825.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "path = r\"D:\\VD_data\"\n",
    "date = \"20230825\"\n",
    "desired_vdids = [\"VD-N1-N-198-O-SE-21-北上出口-彰化\", \"VD-N1-S-369.007-M-Loop\", \"VD-N1-S-369.400-M-Loop\", \"VD-N1-S-370.000-M-Loop\", \"VD-N1-S-371.010-M-Loop\", \"VD-T82-W-14-O-EN-1-Loop\", \"VD-N1-N-33-I-EN-31-五股\"]\n",
    "dataset = path\n",
    "\n",
    "get_vds(date, desired_vdids, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
