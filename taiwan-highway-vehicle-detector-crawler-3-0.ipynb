{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:29.939569Z","iopub.status.busy":"2024-03-02T17:05:29.939174Z","iopub.status.idle":"2024-03-02T17:05:29.949506Z","shell.execute_reply":"2024-03-02T17:05:29.948041Z","shell.execute_reply.started":"2024-03-02T17:05:29.939537Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory 'D:\\VD_data' already exists.\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","import os\n","import requests\n","import concurrent.futures\n","from concurrent.futures import ProcessPoolExecutor\n","from tqdm.notebook import tqdm\n","import gzip\n","import shutil\n","import pandas as pd\n","import xml.etree.ElementTree as ET\n","from IPython.display import display, HTML\n","import zipfile\n","from datetime import datetime, timedelta\n","import pytz\n","\n","# Specify the path to the directory you want to create\n","directory_path = r\"D:\\VD_data\"\n","\n","# Check if the directory already exists\n","if not os.path.exists(directory_path):\n","    # Create the directory if it does not exist\n","    os.makedirs(directory_path)\n","    print(f\"Directory '{directory_path}' created successfully.\")\n","else:\n","    print(f\"Directory '{directory_path}' already exists.\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.141224Z","iopub.status.busy":"2024-03-02T17:05:30.140531Z","iopub.status.idle":"2024-03-02T17:05:30.204085Z","shell.execute_reply":"2024-03-02T17:05:30.203332Z","shell.execute_reply.started":"2024-03-02T17:05:30.141188Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["yesterday date: 20240302\n"]}],"source":["def download_file(url, file_path, log_file_path):\n","    \"\"\"Download a single file, check its size, and return the status.\"\"\"\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(file_path, 'wb') as file:\n","            file.write(response.content)\n","\n","        # Check file size (< 1KB)\n","        if os.path.getsize(file_path) < 1024:\n","            os.remove(file_path)\n","            with open(log_file_path, 'a') as log_file:\n","                log_file.write(f'Deleted: File too small (<1KB): {url}\\n')\n","            print(f'Deleted: {url} (File too small)')\n","            return url, 'small'\n","        #print(f'Downloaded: {url}')\n","        return url, True\n","    except requests.RequestException as e:\n","        with open(log_file_path, 'a') as log_file:\n","            log_file.write(f'Failed to download {url}: {e}\\n')\n","        print(f'Failed to download: {url}')\n","        return url, False\n","\n","def download_files_for_day(directory_path, date, max_concurrent_downloads=10):\n","    print(f\"Starting download for date: {date}\")\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    os.makedirs(compressed_folder_path, exist_ok=True)\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","\n","    # Prepare the download tasks\n","    download_tasks = []\n","    skipped_files = 0\n","    for hour in range(24):\n","        for minute in range(60):\n","            current_time = f'{hour:02d}{minute:02d}'\n","            url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n","            file_path = os.path.join(compressed_folder_path, f'VDLive_{current_time}.xml.gz')\n","            if os.path.exists(file_path):\n","                skipped_files += 1\n","                #print(f'Skipped: {url} (File already exists)')\n","            else:\n","                download_tasks.append((url, file_path))\n","    if skipped_files > 0:\n","        print(f'Skipped {skipped_files} files. (File already exists)')\n","\n","    # Download files concurrently with a progress bar\n","    failed_downloads = []\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor, tqdm(total=len(download_tasks) + skipped_files) as progress:\n","        progress.update(skipped_files)\n","        future_to_url = {executor.submit(download_file, url, file_path, log_file_path): url for url, file_path in download_tasks}\n","        for future in concurrent.futures.as_completed(future_to_url):\n","            url = future_to_url[future]\n","            try:\n","                _, result = future.result()\n","                if result != True:\n","                    failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                progress.update(1)\n","            except Exception as e:\n","                failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                print(f'Error during download: {url}')\n","                progress.update(1)\n","\n","    # Retry failed downloads\n","    if len(failed_downloads) > 0:\n","        print(\"Retrying failed downloads...\")\n","        with tqdm(total=len(failed_downloads)) as progress:\n","            for url, file_path in failed_downloads:\n","                _, result = download_file(url, file_path, log_file_path)\n","                if result != True:\n","                    with open(log_file_path, 'a') as log_file:\n","                        log_file.write(f'Failed to download on retry: {url}\\n')\n","                    print(f'Failed to download on retry: {url}')\n","                progress.update(1)\n","\n","    print(\"Download process completed.\")\n","\n","def decompress_files(directory_path, date):\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    decompressed_folder_path = os.path.join(base_folder_path, 'decompressed')\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","    \n","    os.makedirs(decompressed_folder_path, exist_ok=True)\n","\n","    # List all .xml.gz files in the compressed folder\n","    compressed_files = [f for f in os.listdir(compressed_folder_path) if f.endswith('.xml.gz')]\n","    total_files = len(compressed_files)\n","    print(\"Decompressing xml.gz files...\")\n","\n","    # Progress bar setup\n","    with tqdm(total=total_files) as progress:\n","        for file in compressed_files:\n","            compressed_file_path = os.path.join(compressed_folder_path, file)\n","            decompressed_file_path = os.path.join(decompressed_folder_path, file[:-3])  # Remove .gz from filename\n","\n","            # Skip if decompressed file already exists\n","            if os.path.exists(decompressed_file_path):\n","                print(f'Skipped: {file} (Already decompressed)')\n","                progress.update(1)\n","                continue\n","\n","            try:\n","                # Decompress file\n","                with gzip.open(compressed_file_path, 'rb') as f_in, open(decompressed_file_path, 'wb') as f_out:\n","                    shutil.copyfileobj(f_in, f_out)\n","                #print(f'Decompressed: {file}')\n","            except Exception as e:\n","                with open(log_file_path, 'a') as log_file:\n","                    log_file.write(f'Failed to decompress {file}: {e}\\n')\n","                print(f'Failed to decompress: {file}')\n","            progress.update(1)\n","\n","    print(\"Decompression process completed.\")\n","\n","def parse_xml_file(file_path, namespace):\n","    \"\"\"\n","    Parse an XML file and return a dictionary of data.\n","    \"\"\"\n","    tree = ET.parse(file_path)\n","    root = tree.getroot()\n","\n","    data_dict = {}\n","\n","    def get_nested_element_text(parent, path):\n","        element = parent.find(path, namespace)\n","        return element.text if element is not None else ''\n","\n","    for vdlive in root.findall('.//ns1:VDLive', namespace):\n","        vdid = get_nested_element_text(vdlive, 'ns1:VDID')\n","\n","        if vdid not in data_dict:\n","            data_dict[vdid] = {'VDID': vdid}\n","\n","        for lane in vdlive.findall('.//ns1:Lane', namespace):\n","            lane_id = get_nested_element_text(lane, 'ns1:LaneID')\n","            speed = get_nested_element_text(lane, 'ns1:Speed')\n","            occupancy = get_nested_element_text(lane, 'ns1:Occupancy')\n","\n","            data_dict[vdid].update({\n","                f'L{lane_id}_Speed': speed,\n","                f'L{lane_id}_Occupancy': occupancy,\n","                f'L{lane_id}_S_Volume': 0,\n","                f'L{lane_id}_L_Volume': 0,\n","                f'L{lane_id}_T_Volume': 0,\n","                f'L{lane_id}_S_Vehicle_Speed': 0,\n","                f'L{lane_id}_L_Vehicle_Speed': 0,\n","                f'L{lane_id}_T_Vehicle_Speed': 0\n","            })\n","\n","            for vehicle in lane.findall('.//ns1:Vehicle', namespace):\n","                vehicle_type = get_nested_element_text(vehicle, 'ns1:VehicleType')\n","                volume = get_nested_element_text(vehicle, 'ns1:Volume')\n","                speed2 = get_nested_element_text(vehicle, 'ns1:Speed')\n","\n","                prefix = f'L{lane_id}_{vehicle_type}_'\n","                data_dict[vdid][prefix + 'Volume'] = volume\n","                data_dict[vdid][prefix + 'Vehicle_Speed'] = speed2\n","\n","    return data_dict\n","\n","\n","def convert_xml_to_csv(directory_path, date):\n","    input_dir = os.path.join(directory_path, date, \"decompressed\")\n","    output_dir = os.path.join(directory_path, date, \"csv\")\n","\n","    # Ensure the output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Define your XML namespace\n","    namespace = {'ns1': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n","\n","    # List all XML files in the input directory\n","    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n","    total_files = len(xml_files)\n","\n","    # Initialize progress bar\n","    with tqdm(total=total_files) as progress:\n","        for file_name in xml_files:\n","            try:\n","                # Check if corresponding CSV file already exists\n","                output_file = os.path.join(output_dir, file_name.replace('.xml', '.csv'))\n","                if os.path.exists(output_file):\n","                    # Skip this file if the CSV already exists\n","                    print(f\"Skipping {file_name} as CSV already exists.\")\n","                    progress.update(1)\n","                    continue\n","\n","                # Proceed with conversion if CSV does not exist\n","                file_path = os.path.join(input_dir, file_name)\n","                data_dict = parse_xml_file(file_path, namespace)  # Assuming this is a function you've defined\n","                df = pd.DataFrame(list(data_dict.values()))\n","\n","                # Save to CSV, skipping index\n","                df.to_csv(output_file, index=False)\n","                progress.update(1)  # Update progress after successful conversion\n","            except Exception as e:\n","                # Handle exceptions, such as parse errors or file IO errors\n","                print(f\"Error converting file {file_name}: {e}\")\n","\n","def process_csv_files(directory_path, date):\n","    # Define input and output directories based on the provided date\n","    input_directory = os.path.join(directory_path, date, \"csv\")\n","    output_directory = os.path.join(directory_path, date, \"VDID\")\n","\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(output_directory):\n","        os.makedirs(output_directory)\n","\n","    # Initialize an empty list to store DataFrames\n","    dfs = []\n","\n","    # List all CSV files in the input directory\n","    csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n","    \n","    print(f\"Processing {len(csv_files)} CSV files:\")\n","    \n","    with tqdm(total=len(csv_files), unit='file') as pbar_files:\n","        for i, filename in enumerate(csv_files, start=1):\n","            \n","            try:\n","                # Read the CSV file and insert the 'time' column at the beginning\n","                df = pd.read_csv(os.path.join(input_directory, filename))\n","                df.insert(0, 'file_name', filename)\n","                \n","                # Append the DataFrame to the list\n","                dfs.append(df)\n","                \n","                pbar_files.update(1)\n","            except Exception as e:\n","                # Print an error message and continue processing other files\n","                display(HTML(f'<span style=\"color:red\">Error processing file {filename}: {str(e)}</span>'))\n","\n","    # Concatenate all DataFrames in the list to create the combined DataFrame\n","    combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Group the combined DataFrame by 'VDID'\n","    groups = combined_df.groupby('VDID')\n","    \n","    print(f\"\\nSaving {len(groups)} VDID-specific CSV files:\")\n","    \n","    with tqdm(total=len(groups), unit='VDID') as pbar_vdids:\n","        for i, (vdid, group_df) in enumerate(groups, start=1):\n","            try:\n","                # Save the group-specific data to a CSV file in the output directory\n","                group_df.to_csv(os.path.join(output_directory, f'{vdid}.csv'), index=False)\n","                \n","                pbar_vdids.update(1)\n","            except Exception as e:\n","                # Print an error message if saving fails\n","                display(HTML(f'<span style=\"color:red\">Error saving VDID {vdid}: {str(e)}</span>'))\n","    \n","    print(f\"\\n{len(groups)} VDID-specific CSV files saved.\")\n","\n","def check_files(directory_path, date):\n","    # Define the directory paths based on the input date\n","    csv_directory = os.path.join(directory_path, date, 'csv')\n","    vdid_directory = os.path.join(directory_path, date, 'VDID')\n","    \n","    # Create dictionaries to store the distribution of row counts\n","    csv_row_counts = {}\n","    vdid_row_counts = {}\n","    \n","    # Process CSV files\n","    csv_files = [f for f in os.listdir(csv_directory) if f.endswith(\".csv\")]\n","    csv_progress_bar = tqdm(total=len(csv_files), desc=\"Processing CSV files\")\n","    \n","    for filename in csv_files:\n","        file_path = os.path.join(csv_directory, filename)\n","        # Use 'usecols' to read only the first column\n","        df = pd.read_csv(file_path, usecols=[0])\n","        num_rows = len(df)\n","        \n","        csv_row_counts[num_rows] = csv_row_counts.get(num_rows, 0) + 1\n","        csv_progress_bar.update(1)\n","    \n","    csv_progress_bar.close()\n","    \n","    # Process VDID files if the directory exists\n","    if os.path.exists(vdid_directory):\n","        vdid_files = os.listdir(vdid_directory)\n","        vdid_progress_bar = tqdm(total=len(vdid_files), desc=\"Processing VDID files\")\n","        \n","        for filename in vdid_files:\n","            file_path = os.path.join(vdid_directory, filename)\n","            # Use 'usecols' to read only the first column\n","            df = pd.read_csv(file_path, usecols=[0])\n","            num_rows = len(df)\n","            \n","            vdid_row_counts[num_rows] = vdid_row_counts.get(num_rows, 0) + 1\n","            vdid_progress_bar.update(1)\n","        \n","        vdid_progress_bar.close()\n","    \n","    # Write the results to a log file and print the distributions\n","    log_and_print_results(directory_path, date, csv_row_counts, vdid_row_counts)\n","\n","def log_and_print_results(directory_path, date, csv_row_counts, vdid_row_counts):\n","    # Calculate total rows\n","    total_csv_rows = sum(num_rows * count for num_rows, count in csv_row_counts.items())\n","    total_vdid_rows = sum(num_rows * count for num_rows, count in vdid_row_counts.items())\n","    \n","    # Prepare log file\n","    output_directory = os.path.join(directory_path, date)\n","    log_file_path = os.path.join(output_directory, 'log.txt')\n","    \n","    with open(log_file_path, 'w') as log_file:\n","        log_file.write(\"Distribution of row counts for CSV files:\\n\")\n","        for num_rows, count in sorted(csv_row_counts.items()):\n","            log_file.write(f\"CSV files with {num_rows} rows: {count} files\\n\")\n","        \n","        log_file.write(\"Distribution of row counts for VDID files:\\n\")\n","        for num_rows, count in sorted(vdid_row_counts.items()):\n","            log_file.write(f\"VDID files with {num_rows} rows: {count} files\\n\")\n","        \n","        log_file.write(f\"Total rows in CSV files: {total_csv_rows}\\n\")\n","        log_file.write(f\"Total rows in VDID files: {total_vdid_rows}\\n\")\n","    \n","    # Optionally, print the same information to the console\n","    print_distribution(\"CSV files\", csv_row_counts)\n","    print_distribution(\"VDID files\", vdid_row_counts)\n","    print(f\"Total rows in CSV files: {total_csv_rows}\")\n","    print(f\"Total rows in VDID files: {total_vdid_rows}\")\n","\n","def print_distribution(file_type, row_counts):\n","    print(f\"\\nDistribution of row counts for {file_type}:\")\n","    for num_rows, count in sorted(row_counts.items()):\n","        print(f\"{file_type} with {num_rows} rows: {count} files\")\n","\n","\n","def delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv):\n","    # Define the directory paths based on the input date\n","    compressed_directory = os.path.join(directory_path, date, 'compressed')\n","    decompressed_directory = os.path.join(directory_path, date, 'decompressed')\n","    csv_directory = os.path.join(directory_path, date, 'csv')\n","    \n","    # Helper function to delete files in a directory\n","    def delete_files_in_directory(directory):\n","        if os.path.exists(directory):\n","            file_list = os.listdir(directory)\n","            for file in file_list:\n","                file_path = os.path.join(directory, file)\n","                try:\n","                    if os.path.isfile(file_path):\n","                        os.remove(file_path)\n","                except Exception as e:\n","                    print(f\"Error deleting file: {file_path} ({e})\")\n","        print(f\"Deleted file: {directory}\")\n","    \n","    # Delete files in the specified directories based on the parameter values\n","    if delete_compressed == 1:\n","        delete_files_in_directory(compressed_directory)\n","    \n","    if delete_decompressed == 1:\n","        delete_files_in_directory(decompressed_directory)\n","    \n","    if delete_csv == 1:\n","        delete_files_in_directory(csv_directory)\n","\n","def zip_output(directory_path, date, delete_files_sp_zip=0):\n","    try:\n","        # Construct the path to the directory to zip\n","        dir_to_zip = os.path.join(directory_path, date)\n","        \n","        # Check if the directory exists\n","        if not os.path.exists(dir_to_zip):\n","            print(f\"Directory {dir_to_zip} does not exist.\")\n","            return\n","        \n","        # Output zip file path\n","        output_zip_path = f\"{dir_to_zip}.zip\"\n","        \n","        # Name of the root folder within the zip file\n","        root_folder_name = os.path.basename(dir_to_zip)\n","        \n","        # Count the number of files to zip\n","        file_count = sum([len(files) for r, d, files in os.walk(dir_to_zip)])\n","        \n","        # Notify user about the zipping process\n","        print(f\"Zipping {file_count} files in {dir_to_zip}, please wait...\")\n","        \n","        # Create a zip file and add files to it\n","        with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            for root, dirs, files in os.walk(dir_to_zip):\n","                for file in files:\n","                    # File path to add\n","                    file_path = os.path.join(root, file)\n","                    # Calculate relative path within the zip, including the new root folder\n","                    arcname = os.path.join(root_folder_name, os.path.relpath(file_path, dir_to_zip))\n","                    # Add file to zip\n","                    zipf.write(file_path, arcname)\n","        \n","        # Notify completion\n","        print(f\"Completed zipping directory {dir_to_zip} into {output_zip_path}\")\n","        \n","        # Delete the original directory if delete_files_sp_zip equals 1\n","        if delete_files_sp_zip == 1:\n","            shutil.rmtree(dir_to_zip)\n","            print(f\"Deleted directory {dir_to_zip}\")\n","        \n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","\n","def get_yesterday_date(timezone):\n","    # Convert the current time to the specified timezone\n","    tz = pytz.timezone(timezone)\n","    now_in_timezone = datetime.now(tz)\n","    \n","    # Calculate yesterday's date\n","    yesterday_in_timezone = now_in_timezone - timedelta(days=1)\n","    \n","    # Format yesterday's date as \"YYYYMMDD\"\n","    return yesterday_in_timezone.strftime(\"%Y%m%d\")\n","\n","# Set timezone to Taipei\n","timezone = \"Asia/Taipei\"\n","yesterday_date = get_yesterday_date(timezone)\n","print(f\"yesterday date: {yesterday_date}\")\n","\n","\n","# Main Program\n","\n","def fetch_vd(directory_path, date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    download_files_for_day(directory_path, date, max_concurrent_downloads=5)\n","    decompress_files(directory_path, date)\n","    convert_xml_to_csv(directory_path, date)\n","    process_csv_files(directory_path, date)\n","    check_files(directory_path, date)\n","    delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv)\n","    zip_output(directory_path, date, delete_files_sp_zip)\n","    \n","    \n","def batch_fetch_vd(start_date, num_days_backwards, directory_path, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    # Convert start_date string to datetime object\n","    date_format = \"%Y%m%d\"\n","    current_date = datetime.strptime(start_date, date_format)\n","    \n","    # Iterate backwards from start_date for num_days_backwards\n","    for _ in range(num_days_backwards):\n","        # Convert current_date back to string and call fetch_vd\n","        formatted_date = current_date.strftime(date_format)\n","        fetch_vd(directory_path, formatted_date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip)\n","        \n","        # Decrement the day by one\n","        current_date -= timedelta(days=1)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.205994Z","iopub.status.busy":"2024-03-02T17:05:30.205577Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting download for date: 20240210\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68729774d84342e995565f1ff758222e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"105f1118224e4284ba86e608a94c2b53","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c60b2d311c114ae0b68a82920899de67","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a6dfc5e25a44276a89784375d75ca61","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3629 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4e171b888bf47a8b56e9ee6b188e9ce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3629 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3629 VDID-specific CSV files saved.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd8d686d6bde4125b6188f292b13ddfd","version_major":2,"version_minor":0},"text/plain":["Processing CSV files:   0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2110a6e51e014fe78933b29d7cbb7c97","version_major":2,"version_minor":0},"text/plain":["Processing VDID files:   0%|          | 0/3629 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Distribution of row counts for CSV files:\n","CSV files with 3629 rows: 1440 files\n","\n","Distribution of row counts for VDID files:\n","VDID files with 1440 rows: 3629 files\n","\n","Total rows in CSV files: 5225760\n","Total rows in VDID files: 5225760\n","Total rows in CSV and VDID files are the same.\n","Deleted file: D:\\VD_data\\20240210\\compressed\n","Deleted file: D:\\VD_data\\20240210\\decompressed\n","Zipping 5070 files in D:\\VD_data\\20240210, please wait...\n","Completed zipping directory D:\\VD_data\\20240210 into D:\\VD_data\\20240210.zip\n","Deleted directory D:\\VD_data\\20240210\n","Starting download for date: 20240209\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e45bc9dc410641c2953c6b5524069ca5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#fetch_vd(directory_path, \"20240228\", 1, 1, 0, 1) # date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip\n","\n","#fetch_vd(directory_path, yesterday_date, 1, 1, 0, 0)\n","\n","# Example usage\n","# You need to replace \"your_directory_path_here\" with the actual directory path.\n","# Also, adjust the boolean flags as needed for your use case.\n","batch_fetch_vd(\"20240210\", 10, directory_path, delete_compressed=True, delete_decompressed=True, delete_csv=False, delete_files_sp_zip=True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
