{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taiwan Highway VD data fetching\n",
    "\n",
    "#### Run the code first then execute the xml.gz downloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def download_file(url, file_path, log_file_path):\n",
    "    \"\"\"Download a single file, check its size, and return the status.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        # Check file size (< 1KB)\n",
    "        if os.path.getsize(file_path) < 1024:\n",
    "            os.remove(file_path)\n",
    "            with open(log_file_path, 'a') as log_file:\n",
    "                log_file.write(f'Deleted: File too small (<1KB): {url}\\n')\n",
    "            print(f'Deleted: {url} (File too small)')\n",
    "            return url, 'small'\n",
    "        #print(f'Downloaded: {url}')\n",
    "        return url, True\n",
    "    except requests.RequestException as e:\n",
    "        with open(log_file_path, 'a') as log_file:\n",
    "            log_file.write(f'Failed to download {url}: {e}\\n')\n",
    "        print(f'Failed to download: {url}')\n",
    "        return url, False\n",
    "\n",
    "def download_files_for_day(date, max_concurrent_downloads=10):\n",
    "    print(f\"Starting download for date: {date}\")\n",
    "    base_folder_path = f'D:\\\\VD_data\\\\{date}'\n",
    "    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n",
    "    os.makedirs(compressed_folder_path, exist_ok=True)\n",
    "    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n",
    "\n",
    "    # Prepare the download tasks\n",
    "    download_tasks = []\n",
    "    skipped_files = 0\n",
    "    for hour in range(24):\n",
    "        for minute in range(60):\n",
    "            current_time = f'{hour:02d}{minute:02d}'\n",
    "            url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n",
    "            file_path = os.path.join(compressed_folder_path, f'VDLive_{current_time}.xml.gz')\n",
    "            if os.path.exists(file_path):\n",
    "                skipped_files += 1\n",
    "                #print(f'Skipped: {url} (File already exists)')\n",
    "            else:\n",
    "                download_tasks.append((url, file_path))\n",
    "    if skipped_files > 0:\n",
    "        print(f'Skipped {skipped_files} files. (File already exists)')\n",
    "\n",
    "    # Download files concurrently with a progress bar\n",
    "    failed_downloads = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor, tqdm(total=len(download_tasks) + skipped_files) as progress:\n",
    "        progress.update(skipped_files)\n",
    "        future_to_url = {executor.submit(download_file, url, file_path, log_file_path): url for url, file_path in download_tasks}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                _, result = future.result()\n",
    "                if result != True:\n",
    "                    failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n",
    "                progress.update(1)\n",
    "            except Exception as e:\n",
    "                failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n",
    "                print(f'Error during download: {url}')\n",
    "                progress.update(1)\n",
    "\n",
    "    # Retry failed downloads\n",
    "    if len(failed_downloads) > 0:\n",
    "        print(\"Retrying failed downloads...\")\n",
    "        with tqdm(total=len(failed_downloads)) as progress:\n",
    "            for url, file_path in failed_downloads:\n",
    "                _, result = download_file(url, file_path, log_file_path)\n",
    "                if result != True:\n",
    "                    with open(log_file_path, 'a') as log_file:\n",
    "                        log_file.write(f'Failed to download on retry: {url}\\n')\n",
    "                    print(f'Failed to download on retry: {url}')\n",
    "                progress.update(1)\n",
    "\n",
    "    print(\"Download process completed.\")\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def decompress_files(date):\n",
    "    base_folder_path = f'D:\\\\VD_data\\\\{date}'\n",
    "    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n",
    "    decompressed_folder_path = os.path.join(base_folder_path, 'decompressed')\n",
    "    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n",
    "    \n",
    "    os.makedirs(decompressed_folder_path, exist_ok=True)\n",
    "\n",
    "    # List all .xml.gz files in the compressed folder\n",
    "    compressed_files = [f for f in os.listdir(compressed_folder_path) if f.endswith('.xml.gz')]\n",
    "    total_files = len(compressed_files)\n",
    "    print(\"Decompressing xml.gz files...\")\n",
    "\n",
    "    # Progress bar setup\n",
    "    with tqdm(total=total_files) as progress:\n",
    "        for file in compressed_files:\n",
    "            compressed_file_path = os.path.join(compressed_folder_path, file)\n",
    "            decompressed_file_path = os.path.join(decompressed_folder_path, file[:-3])  # Remove .gz from filename\n",
    "\n",
    "            # Skip if decompressed file already exists\n",
    "            if os.path.exists(decompressed_file_path):\n",
    "                print(f'Skipped: {file} (Already decompressed)')\n",
    "                progress.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Decompress file\n",
    "                with gzip.open(compressed_file_path, 'rb') as f_in, open(decompressed_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "                #print(f'Decompressed: {file}')\n",
    "            except Exception as e:\n",
    "                with open(log_file_path, 'a') as log_file:\n",
    "                    log_file.write(f'Failed to decompress {file}: {e}\\n')\n",
    "                print(f'Failed to decompress: {file}')\n",
    "            progress.update(1)\n",
    "\n",
    "    print(\"Decompression process completed.\")\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def convert_xml_to_csv(date):\n",
    "    input_dir = f\"D:\\\\VD_data\\\\{date}\\\\decompressed\"\n",
    "    output_dir = f\"D:\\\\VD_data\\\\{date}\\\\csv\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define the namespace\n",
    "    namespace = {'ns1': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n",
    "\n",
    "    def get_nested_element_text(parent, path):\n",
    "        element = parent.find(path, namespace)\n",
    "        return element.text if element is not None else ''\n",
    "\n",
    "    # Get the list of XML files\n",
    "    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n",
    "    total_files = len(xml_files)\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=total_files) as progress:\n",
    "        for file_name in xml_files:\n",
    "            try:\n",
    "                tree = ET.parse(os.path.join(input_dir, file_name))\n",
    "                root = tree.getroot()\n",
    "\n",
    "                # Prepare a dictionary to store data for each VDID\n",
    "                data_dict = {}\n",
    "\n",
    "                for vdlive in root.findall('.//ns1:VDLive', namespace):\n",
    "                    vdid = get_nested_element_text(vdlive, 'ns1:VDID')\n",
    "\n",
    "                    if vdid not in data_dict:\n",
    "                        data_dict[vdid] = {\n",
    "                            'VDID': vdid\n",
    "                        }\n",
    "\n",
    "                    for lane in vdlive.findall('.//ns1:Lane', namespace):\n",
    "                        lane_id = get_nested_element_text(lane, 'ns1:LaneID')\n",
    "                        speed = get_nested_element_text(lane, 'ns1:Speed')\n",
    "                        occupancy = get_nested_element_text(lane, 'ns1:Occupancy')\n",
    "\n",
    "                        data_dict[vdid][f'L{lane_id}_Speed'] = speed\n",
    "                        data_dict[vdid][f'L{lane_id}_Occupancy'] = occupancy\n",
    "\n",
    "                        # Initialize volume and speed values for S, L, T\n",
    "                        data_dict[vdid][f'L{lane_id}_S_Volume'] = 0\n",
    "                        data_dict[vdid][f'L{lane_id}_L_Volume'] = 0\n",
    "                        data_dict[vdid][f'L{lane_id}_T_Volume'] = 0\n",
    "                        data_dict[vdid][f'L{lane_id}_S_Vehicle_Speed'] = 0\n",
    "                        data_dict[vdid][f'L{lane_id}_L_Vehicle_Speed'] = 0\n",
    "                        data_dict[vdid][f'L{lane_id}_T_Vehicle_Speed'] = 0\n",
    "\n",
    "                        for vehicle in lane.findall('.//ns1:Vehicle', namespace):\n",
    "                            vehicle_type = get_nested_element_text(vehicle, 'ns1:VehicleType')\n",
    "                            volume = get_nested_element_text(vehicle, 'ns1:Volume')\n",
    "                            speed2 = get_nested_element_text(vehicle, 'ns1:Speed')\n",
    "\n",
    "                            if vehicle_type == 'S':\n",
    "                                data_dict[vdid][f'L{lane_id}_S_Volume'] = volume\n",
    "                                data_dict[vdid][f'L{lane_id}_S_Vehicle_Speed'] = speed2\n",
    "                            elif vehicle_type == 'L':\n",
    "                                data_dict[vdid][f'L{lane_id}_L_Volume'] = volume\n",
    "                                data_dict[vdid][f'L{lane_id}_L_Vehicle_Speed'] = speed2\n",
    "                            elif vehicle_type == 'T':\n",
    "                                data_dict[vdid][f'L{lane_id}_T_Volume'] = volume\n",
    "                                data_dict[vdid][f'L{lane_id}_T_Vehicle_Speed'] = speed2\n",
    "\n",
    "                # Create DataFrame from the data dictionary values\n",
    "                df = pd.DataFrame(list(data_dict.values()))\n",
    "\n",
    "                # Save the modified data to a CSV file\n",
    "                output_file = os.path.join(output_dir, file_name.replace('.xml', '.csv'))\n",
    "                df.to_csv(output_file, index=False)\n",
    "\n",
    "                # Update the progress bar\n",
    "                progress.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting file {file_name}: {e}\")\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def process_csv_files(date):\n",
    "    # Define input and output directories based on the provided date\n",
    "    input_directory = f'D:/VD_data/{date}/csv'\n",
    "    output_directory = f'D:/VD_data/{date}/VDID'\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # List all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"Processing {len(csv_files)} CSV files:\")\n",
    "    \n",
    "    with tqdm(total=len(csv_files), unit='file') as pbar_files:\n",
    "        for i, filename in enumerate(csv_files, start=1):\n",
    "            # Extract the time from the filename (e.g., VDLive_0855.csv -> '0855')\n",
    "            time = filename.split('_')[1].split('.')[0]\n",
    "            \n",
    "            try:\n",
    "                # Read the CSV file and insert the 'time' column at the beginning\n",
    "                df = pd.read_csv(os.path.join(input_directory, filename))\n",
    "                df.insert(0, 'time', time)\n",
    "                \n",
    "                # Append the DataFrame to the list\n",
    "                dfs.append(df)\n",
    "                \n",
    "                pbar_files.update(1)\n",
    "            except Exception as e:\n",
    "                # Print an error message and continue processing other files\n",
    "                display(HTML(f'<span style=\"color:red\">Error processing file {filename}: {str(e)}</span>'))\n",
    "\n",
    "    # Concatenate all DataFrames in the list to create the combined DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Group the combined DataFrame by 'VDID'\n",
    "    groups = combined_df.groupby('VDID')\n",
    "    \n",
    "    print(f\"\\nSaving {len(groups)} VDID-specific CSV files:\")\n",
    "    \n",
    "    with tqdm(total=len(groups), unit='VDID') as pbar_vdids:\n",
    "        for i, (vdid, group_df) in enumerate(groups, start=1):\n",
    "            try:\n",
    "                # Save the group-specific data to a CSV file in the output directory\n",
    "                group_df.to_csv(os.path.join(output_directory, f'{vdid}.csv'), index=False)\n",
    "                \n",
    "                pbar_vdids.update(1)\n",
    "            except Exception as e:\n",
    "                # Print an error message if saving fails\n",
    "                display(HTML(f'<span style=\"color:red\">Error saving VDID {vdid}: {str(e)}</span>'))\n",
    "    \n",
    "    print(f\"\\n{len(groups)} VDID-specific CSV files saved.\")\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def check_files(date):\n",
    "    # Define the directory paths based on the input date\n",
    "    csv_directory = os.path.join(r'D:\\VD_data', date, 'csv')\n",
    "    vdid_directory = os.path.join(r'D:\\VD_data', date, 'VDID')\n",
    "    \n",
    "    # Create dictionaries to store the distribution of row counts for CSV files and VDID files\n",
    "    csv_row_counts = {}\n",
    "    vdid_row_counts = {}\n",
    "    \n",
    "    # Get the total number of CSV files in the CSV directory\n",
    "    total_csv_files = len([filename for filename in os.listdir(csv_directory) if filename.endswith(\".csv\")])\n",
    "    \n",
    "    # Create a progress bar for processing CSV files\n",
    "    csv_progress_bar = tqdm(total=total_csv_files, desc=\"Processing CSV files\")\n",
    "    \n",
    "    # Iterate through CSV files in the CSV directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get the number of rows in the DataFrame\n",
    "            num_rows = len(df)\n",
    "            \n",
    "            # Update the csv_row_counts dictionary\n",
    "            if num_rows in csv_row_counts:\n",
    "                csv_row_counts[num_rows] += 1\n",
    "            else:\n",
    "                csv_row_counts[num_rows] = 1\n",
    "            \n",
    "            # Update the CSV progress bar\n",
    "            csv_progress_bar.update(1)\n",
    "    \n",
    "    # Close the CSV progress bar\n",
    "    csv_progress_bar.close()\n",
    "    \n",
    "    # Survey the 'VDID' directory\n",
    "    if os.path.exists(vdid_directory):\n",
    "        vdid_files = os.listdir(vdid_directory)\n",
    "        \n",
    "        # Create a progress bar for processing VDID files\n",
    "        vdid_progress_bar = tqdm(total=len(vdid_files), desc=\"Processing VDID files\")\n",
    "        \n",
    "        # Iterate through VDID files in the VDID directory\n",
    "        for filename in vdid_files:\n",
    "            file_path = os.path.join(vdid_directory, filename)\n",
    "            \n",
    "            # Read the VDID file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get the number of rows in the DataFrame\n",
    "            num_rows = len(df)\n",
    "            \n",
    "            # Update the vdid_row_counts dictionary\n",
    "            if num_rows in vdid_row_counts:\n",
    "                vdid_row_counts[num_rows] += 1\n",
    "            else:\n",
    "                vdid_row_counts[num_rows] = 1\n",
    "            \n",
    "            # Update the VDID progress bar\n",
    "            vdid_progress_bar.update(1)\n",
    "        \n",
    "        # Close the VDID progress bar\n",
    "        vdid_progress_bar.close()\n",
    "    \n",
    "    # Calculate the total rows for CSV and VDID files\n",
    "    total_csv_rows = sum(num_rows * count for num_rows, count in csv_row_counts.items())\n",
    "    total_vdid_rows = sum(num_rows * count for num_rows, count in vdid_row_counts.items())\n",
    "    \n",
    "    # Define the output directory and log file path\n",
    "    output_directory = os.path.join(r'D:\\VD_data', date)\n",
    "    log_file_path = os.path.join(output_directory, 'log.txt')\n",
    "    \n",
    "    # Write the distribution of row counts for CSV files to the log file\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        log_file.write(\"Distribution of row counts for CSV files:\\n\")\n",
    "        for num_rows, count in sorted(csv_row_counts.items()):\n",
    "            log_file.write(f\"CSV files with {num_rows} rows: {count} files\\n\")\n",
    "        \n",
    "        # Write the distribution of row counts for VDID files to the log file\n",
    "        log_file.write(\"Distribution of row counts for VDID files:\\n\")\n",
    "        for num_rows, count in sorted(vdid_row_counts.items()):\n",
    "            log_file.write(f\"VDID files with {num_rows} rows: {count} files\\n\")\n",
    "        \n",
    "        # Write the total rows for CSV and VDID files\n",
    "        log_file.write(f\"Total rows in CSV files: {total_csv_rows}\\n\")\n",
    "        log_file.write(f\"Total rows in VDID files: {total_vdid_rows}\\n\")\n",
    "    \n",
    "    # Print the distribution of row counts for CSV files\n",
    "    print()\n",
    "    print(\"Distribution of row counts for CSV files:\")\n",
    "    for num_rows, count in sorted(csv_row_counts.items()):\n",
    "        print(f\"CSV files with {num_rows} rows: {count} files\")\n",
    "    \n",
    "    # Print the distribution of row counts for VDID files\n",
    "    print()\n",
    "    print(\"Distribution of row counts for VDID files:\")\n",
    "    for num_rows, count in sorted(vdid_row_counts.items()):\n",
    "        print(f\"VDID files with {num_rows} rows: {count} files\")\n",
    "    \n",
    "    # Print the total rows for CSV and VDID files\n",
    "    print()\n",
    "    print(f\"Total rows in CSV files: {total_csv_rows}\")\n",
    "    print(f\"Total rows in VDID files: {total_vdid_rows}\")\n",
    "    \n",
    "    # Compare and report any differences in total rows\n",
    "    if total_csv_rows == total_vdid_rows:\n",
    "        print(\"Total rows in CSV and VDID files are the same.\")\n",
    "    else:\n",
    "        print(\"Total rows in CSV and VDID files are different.\")\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "def delete_files(date, delete_compressed, delete_decompressed, delete_csv):\n",
    "    # Define the directory paths based on the input date\n",
    "    compressed_directory = os.path.join(r'D:\\VD_data', date, 'compressed')\n",
    "    decompressed_directory = os.path.join(r'D:\\VD_data', date, 'decompressed')\n",
    "    csv_directory = os.path.join(r'D:\\VD_data', date, 'csv')\n",
    "    \n",
    "    # Helper function to delete files in a directory\n",
    "    def delete_files_in_directory(directory):\n",
    "        if os.path.exists(directory):\n",
    "            file_list = os.listdir(directory)\n",
    "            for file in file_list:\n",
    "                file_path = os.path.join(directory, file)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting file: {file_path} ({e})\")\n",
    "        print(f\"Deleted file: {directory}\")\n",
    "    \n",
    "    # Delete files in the specified directories based on the parameter values\n",
    "    if delete_compressed == 1:\n",
    "        delete_files_in_directory(compressed_directory)\n",
    "    \n",
    "    if delete_decompressed == 1:\n",
    "        delete_files_in_directory(decompressed_directory)\n",
    "    \n",
    "    if delete_csv == 1:\n",
    "        delete_files_in_directory(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download for date: 20240125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64b3a81563f49d2b2cd2e7926da5d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download process completed.\n",
      "Decompressing xml.gz files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d7eebad83f4b8189c6829c526281b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompression process completed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4a1a60f30427ebb115849a7f224aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1440 CSV files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca52d5e0f214af496af4e1a80e850e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 3629 VDID-specific CSV files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f633d98ba1844f19157a86fcf15b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3629 [00:00<?, ?VDID/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3629 VDID-specific CSV files saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd65d34b84d43c99e761941d3526ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV files:   0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dcf781eb4c4f45a5d82ab0bda5932c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing VDID files:   0%|          | 0/3629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of row counts for CSV files:\n",
      "CSV files with 3628 rows: 537 files\n",
      "CSV files with 3629 rows: 903 files\n",
      "\n",
      "Distribution of row counts for VDID files:\n",
      "VDID files with 903 rows: 1 files\n",
      "VDID files with 1440 rows: 3628 files\n",
      "\n",
      "Total rows in CSV files: 5225223\n",
      "Total rows in VDID files: 5225223\n",
      "Total rows in CSV and VDID files are the same.\n",
      "Deleted file: D:\\VD_data\\20240125\\compressed\n",
      "Deleted file: D:\\VD_data\\20240125\\decompressed\n"
     ]
    }
   ],
   "source": [
    "def fetch_vd(date, delete_compressed, delete_decompressed, delete_csv):\n",
    "    download_files_for_day(date, max_concurrent_downloads=5)\n",
    "    decompress_files(date)\n",
    "    convert_xml_to_csv(date)\n",
    "    process_csv_files(date)\n",
    "    check_files(date)\n",
    "    delete_files(date, delete_compressed, delete_decompressed, delete_csv)\n",
    "\n",
    "\n",
    "fetch_vd(\"20240125\", 1, 1, 0) # date, delete_compressed, delete_decompressed, delete_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
