{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:29.939569Z","iopub.status.busy":"2024-03-02T17:05:29.939174Z","iopub.status.idle":"2024-03-02T17:05:29.949506Z","shell.execute_reply":"2024-03-02T17:05:29.948041Z","shell.execute_reply.started":"2024-03-02T17:05:29.939537Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","import requests\n","import concurrent.futures\n","from tqdm.notebook import tqdm\n","import gzip\n","import shutil\n","import xml.etree.ElementTree as ET\n","import zipfile\n","from datetime import datetime, timedelta\n","import pytz\n","import gc\n","\n","# Specify the path to the directory you want to create\n","directory_path = r\"D:\\VD_data\"\n","\n","# Check if the directory already exists\n","if not os.path.exists(directory_path):\n","    # Create the directory if it does not exist\n","    os.makedirs(directory_path)\n","    print(f\"Directory '{directory_path}' created successfully.\")\n","else:\n","    print(f\"Directory '{directory_path}' already exists.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.141224Z","iopub.status.busy":"2024-03-02T17:05:30.140531Z","iopub.status.idle":"2024-03-02T17:05:30.204085Z","shell.execute_reply":"2024-03-02T17:05:30.203332Z","shell.execute_reply.started":"2024-03-02T17:05:30.141188Z"},"trusted":true},"outputs":[],"source":["def download_file(url, file_path, log_file_path):\n","    \"\"\"Download a single file, check its size, and return the status.\"\"\"\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(file_path, 'wb') as file:\n","            file.write(response.content)\n","\n","        # Check file size (< 1KB)\n","        if os.path.getsize(file_path) < 1024:\n","            os.remove(file_path)\n","            with open(log_file_path, 'a') as log_file:\n","                log_file.write(f'Deleted: File too small (<1KB): {url}\\n')\n","            print(f'Deleted: {url} (File too small)')\n","            return url, 'small'\n","        #print(f'Downloaded: {url}')\n","        return url, True\n","    except requests.RequestException as e:\n","        with open(log_file_path, 'a') as log_file:\n","            log_file.write(f'Failed to download {url}: {e}\\n')\n","        print(f'Failed to download: {url}')\n","        return url, False\n","\n","def download_files_for_day(directory_path, date, max_concurrent_downloads=10):\n","    print(f\"Starting download for date: {date}\")\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    os.makedirs(compressed_folder_path, exist_ok=True)\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","\n","    # Prepare the download tasks\n","    download_tasks = []\n","    skipped_files = 0\n","    for hour in range(24):\n","        for minute in range(60):\n","            current_time = f'{hour:02d}{minute:02d}'\n","            url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n","            file_path = os.path.join(compressed_folder_path, f'VDLive_{current_time}.xml.gz')\n","            if os.path.exists(file_path):\n","                skipped_files += 1\n","                #print(f'Skipped: {url} (File already exists)')\n","            else:\n","                download_tasks.append((url, file_path))\n","    if skipped_files > 0:\n","        print(f'Skipped {skipped_files} files. (File already exists)')\n","\n","    # Download files concurrently with a progress bar\n","    failed_downloads = []\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor, tqdm(total=len(download_tasks) + skipped_files) as progress:\n","        progress.update(skipped_files)\n","        future_to_url = {executor.submit(download_file, url, file_path, log_file_path): url for url, file_path in download_tasks}\n","        for future in concurrent.futures.as_completed(future_to_url):\n","            url = future_to_url[future]\n","            try:\n","                _, result = future.result()\n","                if result != True:\n","                    failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                progress.update(1)\n","            except Exception as e:\n","                failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                print(f'Error during download: {url}')\n","                progress.update(1)\n","\n","    # Retry failed downloads\n","    if len(failed_downloads) > 0:\n","        print(\"Retrying failed downloads...\")\n","        with tqdm(total=len(failed_downloads)) as progress:\n","            for url, file_path in failed_downloads:\n","                _, result = download_file(url, file_path, log_file_path)\n","                if result != True:\n","                    with open(log_file_path, 'a') as log_file:\n","                        log_file.write(f'Failed to download on retry: {url}\\n')\n","                    print(f'Failed to download on retry: {url}')\n","                progress.update(1)\n","\n","    print(\"Download process completed.\")\n","\n","def decompress_files(directory_path, date):\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    decompressed_folder_path = os.path.join(base_folder_path, 'decompressed')\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","    \n","    os.makedirs(decompressed_folder_path, exist_ok=True)\n","\n","    # List all .xml.gz files in the compressed folder\n","    compressed_files = [f for f in os.listdir(compressed_folder_path) if f.endswith('.xml.gz')]\n","    total_files = len(compressed_files)\n","    print(\"Decompressing xml.gz files...\")\n","\n","    # Progress bar setup\n","    with tqdm(total=total_files) as progress:\n","        for file in compressed_files:\n","            compressed_file_path = os.path.join(compressed_folder_path, file)\n","            decompressed_file_path = os.path.join(decompressed_folder_path, file[:-3])  # Remove .gz from filename\n","\n","            # Skip if decompressed file already exists\n","            if os.path.exists(decompressed_file_path):\n","                print(f'Skipped: {file} (Already decompressed)')\n","                progress.update(1)\n","                continue\n","\n","            try:\n","                # Decompress file\n","                with gzip.open(compressed_file_path, 'rb') as f_in, open(decompressed_file_path, 'wb') as f_out:\n","                    shutil.copyfileobj(f_in, f_out)\n","                #print(f'Decompressed: {file}')\n","            except Exception as e:\n","                with open(log_file_path, 'a') as log_file:\n","                    log_file.write(f'Failed to decompress {file}: {e}\\n')\n","                print(f'Failed to decompress: {file}')\n","            progress.update(1)\n","\n","    print(\"Decompression process completed.\")\n","\n","#############################################################################################################\n","\n","def parse_xml_file(file_path, namespace):\n","    \"\"\"\n","    Parse an XML file using iterparse for better memory management and return a list of dictionaries\n","    of data, flattened for easy CSV conversion.\n","    \"\"\"\n","    data_dict = {}\n","\n","    for event, elem in ET.iterparse(file_path, events=('end',)):\n","        if elem.tag == f\"{{{namespace['ns1']}}}VDLive\":\n","            vdid = elem.find(f\".//{{{namespace['ns1']}}}VDID\").text if elem.find(f\".//{{{namespace['ns1']}}}VDID\") is not None else ''\n","\n","            if vdid not in data_dict:\n","                data_dict[vdid] = {}\n","\n","            for lane in elem.findall(f\".//{{{namespace['ns1']}}}Lane\"):\n","                lane_id = lane.find(f\".//{{{namespace['ns1']}}}LaneID\").text if lane.find(f\".//{{{namespace['ns1']}}}LaneID\") is not None else ''\n","                speed = lane.find(f\".//{{{namespace['ns1']}}}Speed\").text if lane.find(f\".//{{{namespace['ns1']}}}Speed\") is not None else ''\n","                occupancy = lane.find(f\".//{{{namespace['ns1']}}}Occupancy\").text if lane.find(f\".//{{{namespace['ns1']}}}Occupancy\") is not None else ''\n","\n","                lane_key = f'L{lane_id}'\n","                if lane_key not in data_dict[vdid]:\n","                    data_dict[vdid][lane_key] = {}\n","\n","                data_dict[vdid][lane_key].update({\n","                    'Speed': speed,\n","                    'Occupancy': occupancy,\n","                })\n","\n","                for vehicle in lane.findall(f\".//{{{namespace['ns1']}}}Vehicle\"):\n","                    vehicle_type = vehicle.find(f\".//{{{namespace['ns1']}}}VehicleType\").text if vehicle.find(f\".//{{{namespace['ns1']}}}VehicleType\") is not None else ''\n","                    volume = vehicle.find(f\".//{{{namespace['ns1']}}}Volume\").text if vehicle.find(f\".//{{{namespace['ns1']}}}Volume\") is not None else ''\n","                    speed2 = vehicle.find(f\".//{{{namespace['ns1']}}}Speed\").text if vehicle.find(f\".//{{{namespace['ns1']}}}Speed\") is not None else ''\n","\n","                    data_dict[vdid][lane_key].update({\n","                        f'{vehicle_type}_Volume': volume,\n","                        f'{vehicle_type}_Vehicle_Speed': speed2,\n","                    })\n","\n","            # Clear the element to free memory\n","            elem.clear()\n","\n","    # Flattening the data structure for CSV conversion\n","    flattened_data = []\n","    for vdid, lanes in data_dict.items():\n","        row = {'VDID': vdid}\n","        for lane_id, details in lanes.items():\n","            for key, value in details.items():\n","                row[f'{lane_id}_{key}'] = value\n","        flattened_data.append(row)\n","\n","    return flattened_data\n","\n","\n","def process_file(file_name, input_dir, output_dir, namespace):\n","    try:\n","        # Construct full paths for input and output files\n","        file_path = os.path.join(input_dir, file_name)\n","        output_file = os.path.join(output_dir, file_name.replace('.xml', '.csv'))\n","\n","        # Check if corresponding CSV file already exists\n","        if os.path.exists(output_file):\n","            print(f\"Skipping {file_name} as CSV already exists.\")\n","            return\n","\n","        # Convert XML to DataFrame\n","        flattened_data = parse_xml_file(file_path, namespace)\n","        df = pd.DataFrame(flattened_data)\n","\n","        # Save to CSV, skipping index\n","        df.to_csv(output_file, index=False)\n","        return file_name\n","    except Exception as e:\n","        print(f\"Error converting file {file_name}: {e}\")\n","        return None\n","\n","def convert_xml_to_csv(directory_path, date):\n","    # Implementation remains mostly the same as before\n","    # The function now defaults to using 16 worker threads\n","\n","    input_dir = os.path.join(directory_path, date, \"decompressed\")\n","    output_dir = os.path.join(directory_path, date, \"csv\")\n","\n","    # Ensure the output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Define your XML namespace\n","    namespace = {'ns1': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n","\n","    # List all XML files in the input directory\n","    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n","    total_files = len(xml_files)\n","\n","    # Use ThreadPoolExecutor with a specified number of workers to process files concurrently\n","    with tqdm(total=total_files) as progress:\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","            # Prepare futures for all files\n","            futures = [executor.submit(process_file, file_name, input_dir, output_dir, namespace) for file_name in xml_files]\n","\n","            # Process futures as they complete\n","            for future in concurrent.futures.as_completed(futures):\n","                result = future.result()\n","                if result:\n","                    progress.update(1)\n","\n","\n","\n","#############################################################################################################\n","\n","def process_csv_files(directory_path, date):\n","    # Define input and output directories based on the provided date\n","    input_directory = os.path.join(directory_path, date, \"csv\")\n","    output_directory = os.path.join(directory_path, date, \"VDID\")\n","\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(output_directory):\n","        os.makedirs(output_directory)\n","\n","    # Initialize an empty list to store DataFrames\n","    dfs = []\n","\n","    # List all CSV files in the input directory\n","    csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n","    \n","    print(f\"Processing {len(csv_files)} CSV files:\")\n","    \n","    with tqdm(total=len(csv_files), unit='file') as pbar_files:\n","        for filename in csv_files:\n","            try:\n","                # Read the CSV file and insert the 'file_name' column at the beginning\n","                df = pd.read_csv(os.path.join(input_directory, filename))\n","                df.insert(0, 'file_name', filename)\n","                \n","                # Append the DataFrame to the list\n","                dfs.append(df)\n","                \n","                pbar_files.update(1)\n","            except Exception as e:\n","                # Print an error message and continue processing other files\n","                print(f'Error processing file {filename}: {e}')\n","\n","    # Concatenate all DataFrames in the list to create the combined DataFrame\n","    combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Clear the list of individual DataFrames to release memory\n","    dfs.clear()\n","    gc.collect()  # Manually trigger garbage collection\n","\n","    # Group the combined DataFrame by 'VDID'\n","    groups = combined_df.groupby('VDID')\n","    \n","    print(f\"\\nSaving {len(groups)} VDID-specific CSV files:\")\n","    \n","    with tqdm(total=len(groups), unit='VDID') as pbar_vdids:\n","        for vdid, group_df in groups:\n","            try:\n","                # Save the group-specific data to a CSV file in the output directory\n","                group_df.to_csv(os.path.join(output_directory, f'{vdid}.csv'), index=False)\n","                \n","                pbar_vdids.update(1)\n","            except Exception as e:\n","                # Print an error message if saving fails\n","                print(f'Error saving VDID {vdid}: {e}')\n","    \n","    print(f\"\\n{len(groups)} VDID-specific CSV files saved.\")\n","\n","    # Clear variables holding large data and manually collect garbage again\n","    del combined_df, groups\n","    gc.collect()\n","\n","\n","\n","def delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv):\n","    # Define the directory paths based on the input date\n","    compressed_directory = os.path.join(directory_path, date, 'compressed')\n","    decompressed_directory = os.path.join(directory_path, date, 'decompressed')\n","    csv_directory = os.path.join(directory_path, date, 'csv')\n","    \n","    # Helper function to delete files in a directory\n","    def delete_files_in_directory(directory):\n","        if os.path.exists(directory):\n","            file_list = os.listdir(directory)\n","            for file in file_list:\n","                file_path = os.path.join(directory, file)\n","                try:\n","                    if os.path.isfile(file_path):\n","                        os.remove(file_path)\n","                except Exception as e:\n","                    print(f\"Error deleting file: {file_path} ({e})\")\n","        print(f\"Deleted file: {directory}\")\n","    \n","    # Delete files in the specified directories based on the parameter values\n","    if delete_compressed == 1:\n","        delete_files_in_directory(compressed_directory)\n","    \n","    if delete_decompressed == 1:\n","        delete_files_in_directory(decompressed_directory)\n","    \n","    if delete_csv == 1:\n","        delete_files_in_directory(csv_directory)\n","\n","def zip_output(directory_path, date, delete_files_sp_zip=0):\n","    try:\n","        # Construct the path to the directory to zip\n","        dir_to_zip = os.path.join(directory_path, date)\n","        \n","        # Check if the directory exists\n","        if not os.path.exists(dir_to_zip):\n","            print(f\"Directory {dir_to_zip} does not exist.\")\n","            return\n","        \n","        # Output zip file path\n","        output_zip_path = f\"{dir_to_zip}.zip\"\n","        \n","        # Name of the root folder within the zip file\n","        root_folder_name = os.path.basename(dir_to_zip)\n","        \n","        # Gather all files to zip\n","        files_to_zip = []\n","        for root, dirs, files in os.walk(dir_to_zip):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.join(root_folder_name, os.path.relpath(file_path, dir_to_zip))\n","                files_to_zip.append((file_path, arcname))\n","        \n","        file_count = len(files_to_zip)\n","        \n","        # Notify user about the zipping process\n","        print(f\"Zipping {file_count} files in {dir_to_zip}, please wait...\")\n","        \n","        with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            # Wrap the files_to_zip list with tqdm for a progress bar\n","            for file_path, arcname in tqdm(files_to_zip, desc=\"Zipping\"):\n","                zipf.write(file_path, arcname)\n","        \n","        # Notify completion\n","        print(f\"Completed zipping directory {dir_to_zip} into {output_zip_path}\")\n","        \n","        # Delete the original directory if delete_files_sp_zip equals 1\n","        if delete_files_sp_zip == 1:\n","            shutil.rmtree(dir_to_zip)\n","            print(f\"Deleted directory {dir_to_zip}\")\n","        \n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","\n","\n","def get_yesterday_date(timezone):\n","    # Convert the current time to the specified timezone\n","    tz = pytz.timezone(timezone)\n","    now_in_timezone = datetime.now(tz)\n","    \n","    # Calculate yesterday's date\n","    yesterday_in_timezone = now_in_timezone - timedelta(days=1)\n","    \n","    # Format yesterday's date as \"YYYYMMDD\"\n","    return yesterday_in_timezone.strftime(\"%Y%m%d\")\n","\n","# Set timezone to Taipei\n","timezone = \"Asia/Taipei\"\n","yesterday_date = get_yesterday_date(timezone)\n","print(f\"yesterday date: {yesterday_date}\")\n","\n","\n","# Main Program\n","\n","def fetch_vd(directory_path, date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    download_files_for_day(directory_path, date, max_concurrent_downloads=10)\n","    decompress_files(directory_path, date)\n","    convert_xml_to_csv(directory_path, date)\n","    process_csv_files(directory_path, date)\n","    delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv)\n","    zip_output(directory_path, date, delete_files_sp_zip)\n","    \n","    \n","def batch_fetch_vd(start_date, num_days_backwards, directory_path, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    # Convert start_date string to datetime object\n","    date_format = \"%Y%m%d\"\n","    current_date = datetime.strptime(start_date, date_format)\n","    \n","    # Iterate backwards from start_date for num_days_backwards\n","    for _ in range(num_days_backwards):\n","        # Convert current_date back to string and call fetch_vd\n","        formatted_date = current_date.strftime(date_format)\n","        fetch_vd(directory_path, formatted_date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip)\n","        \n","        # Decrement the day by one\n","        current_date -= timedelta(days=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.205994Z","iopub.status.busy":"2024-03-02T17:05:30.205577Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#fetch_vd(directory_path, \"20240228\", 1, 1, 0, 1) # date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip\n","\n","#fetch_vd(directory_path, yesterday_date, 1, 1, 0, 0)\n","\n","# Example usage\n","# You need to replace \"your_directory_path_here\" with the actual directory path.\n","# Also, adjust the boolean flags as needed for your use case.\n","batch_fetch_vd(\"20240205\", 5, directory_path, delete_compressed=True, delete_decompressed=True, delete_csv=False, delete_files_sp_zip=True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
