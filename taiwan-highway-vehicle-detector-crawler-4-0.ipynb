{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:29.939569Z","iopub.status.busy":"2024-03-02T17:05:29.939174Z","iopub.status.idle":"2024-03-02T17:05:29.949506Z","shell.execute_reply":"2024-03-02T17:05:29.948041Z","shell.execute_reply.started":"2024-03-02T17:05:29.939537Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory 'D:\\VD_data' already exists.\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","import requests\n","import concurrent.futures\n","from tqdm.notebook import tqdm\n","import gzip\n","import shutil\n","import xml.etree.ElementTree as ET\n","import zipfile\n","from datetime import datetime, timedelta\n","import pytz\n","import gc\n","\n","# Specify the path to the directory you want to create\n","directory_path = r\"D:\\VD_data\"\n","\n","# Check if the directory already exists\n","if not os.path.exists(directory_path):\n","    # Create the directory if it does not exist\n","    os.makedirs(directory_path)\n","    print(f\"Directory '{directory_path}' created successfully.\")\n","else:\n","    print(f\"Directory '{directory_path}' already exists.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.141224Z","iopub.status.busy":"2024-03-02T17:05:30.140531Z","iopub.status.idle":"2024-03-02T17:05:30.204085Z","shell.execute_reply":"2024-03-02T17:05:30.203332Z","shell.execute_reply.started":"2024-03-02T17:05:30.141188Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["yesterday date: 20240313\n"]}],"source":["def download_file(url, file_path, log_file_path):\n","    \"\"\"Download a single file, check its size, and return the status.\"\"\"\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an error for bad status codes\n","        with open(file_path, 'wb') as file:\n","            file.write(response.content)\n","\n","        # Check file size (< 1KB)\n","        if os.path.getsize(file_path) < 1024:\n","            os.remove(file_path)\n","            with open(log_file_path, 'a') as log_file:\n","                log_file.write(f'Deleted: File too small (<1KB): {url}\\n')\n","            print(f'Deleted: {url} (File too small)')\n","            return url, 'small'\n","        #print(f'Downloaded: {url}')\n","        return url, True\n","    except requests.RequestException as e:\n","        with open(log_file_path, 'a') as log_file:\n","            log_file.write(f'Failed to download {url}: {e}\\n')\n","        print(f'Failed to download: {url}')\n","        return url, False\n","\n","def download_files_for_day(directory_path, date, max_concurrent_downloads=10):\n","    print(f\"Starting download for date: {date}\")\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    os.makedirs(compressed_folder_path, exist_ok=True)\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","\n","    # Prepare the download tasks\n","    download_tasks = []\n","    skipped_files = 0\n","    for hour in range(24):\n","        for minute in range(60):\n","            current_time = f'{hour:02d}{minute:02d}'\n","            url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n","            file_path = os.path.join(compressed_folder_path, f'VDLive_{current_time}.xml.gz')\n","            if os.path.exists(file_path):\n","                skipped_files += 1\n","                #print(f'Skipped: {url} (File already exists)')\n","            else:\n","                download_tasks.append((url, file_path))\n","    if skipped_files > 0:\n","        print(f'Skipped {skipped_files} files. (File already exists)')\n","\n","    # Download files concurrently with a progress bar\n","    failed_downloads = []\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor, tqdm(total=len(download_tasks) + skipped_files) as progress:\n","        progress.update(skipped_files)\n","        future_to_url = {executor.submit(download_file, url, file_path, log_file_path): url for url, file_path in download_tasks}\n","        for future in concurrent.futures.as_completed(future_to_url):\n","            url = future_to_url[future]\n","            try:\n","                _, result = future.result()\n","                if result != True:\n","                    failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                progress.update(1)\n","            except Exception as e:\n","                failed_downloads.append((url, os.path.join(compressed_folder_path, url.split('/')[-1])))\n","                print(f'Error during download: {url}')\n","                progress.update(1)\n","\n","    # Retry failed downloads\n","    if len(failed_downloads) > 0:\n","        print(\"Retrying failed downloads...\")\n","        with tqdm(total=len(failed_downloads)) as progress:\n","            for url, file_path in failed_downloads:\n","                _, result = download_file(url, file_path, log_file_path)\n","                if result != True:\n","                    with open(log_file_path, 'a') as log_file:\n","                        log_file.write(f'Failed to download on retry: {url}\\n')\n","                    print(f'Failed to download on retry: {url}')\n","                progress.update(1)\n","\n","    print(\"Download process completed.\")\n","\n","def decompress_files(directory_path, date):\n","    base_folder_path = os.path.join(directory_path, date)\n","    compressed_folder_path = os.path.join(base_folder_path, 'compressed')\n","    decompressed_folder_path = os.path.join(base_folder_path, 'decompressed')\n","    log_file_path = os.path.join(base_folder_path, 'download_issues.log')\n","    \n","    os.makedirs(decompressed_folder_path, exist_ok=True)\n","\n","    # List all .xml.gz files in the compressed folder\n","    compressed_files = [f for f in os.listdir(compressed_folder_path) if f.endswith('.xml.gz')]\n","    total_files = len(compressed_files)\n","    print(\"Decompressing xml.gz files...\")\n","\n","    # Progress bar setup\n","    with tqdm(total=total_files) as progress:\n","        for file in compressed_files:\n","            compressed_file_path = os.path.join(compressed_folder_path, file)\n","            decompressed_file_path = os.path.join(decompressed_folder_path, file[:-3])  # Remove .gz from filename\n","\n","            # Skip if decompressed file already exists\n","            if os.path.exists(decompressed_file_path):\n","                print(f'Skipped: {file} (Already decompressed)')\n","                progress.update(1)\n","                continue\n","\n","            try:\n","                # Decompress file\n","                with gzip.open(compressed_file_path, 'rb') as f_in, open(decompressed_file_path, 'wb') as f_out:\n","                    shutil.copyfileobj(f_in, f_out)\n","                #print(f'Decompressed: {file}')\n","            except Exception as e:\n","                with open(log_file_path, 'a') as log_file:\n","                    log_file.write(f'Failed to decompress {file}: {e}\\n')\n","                print(f'Failed to decompress: {file}')\n","            progress.update(1)\n","\n","    print(\"Decompression process completed.\")\n","\n","#############################################################################################################\n","\n","def parse_xml_file(file_path, namespace):\n","    \"\"\"\n","    Parse an XML file using iterparse for better memory management and return a list of dictionaries\n","    of data, flattened for easy CSV conversion.\n","    \"\"\"\n","    data_dict = {}\n","\n","    for event, elem in ET.iterparse(file_path, events=('end',)):\n","        if elem.tag == f\"{{{namespace['ns1']}}}VDLive\":\n","            vdid = elem.find(f\".//{{{namespace['ns1']}}}VDID\").text if elem.find(f\".//{{{namespace['ns1']}}}VDID\") is not None else ''\n","\n","            if vdid not in data_dict:\n","                data_dict[vdid] = {}\n","\n","            for lane in elem.findall(f\".//{{{namespace['ns1']}}}Lane\"):\n","                lane_id = lane.find(f\".//{{{namespace['ns1']}}}LaneID\").text if lane.find(f\".//{{{namespace['ns1']}}}LaneID\") is not None else ''\n","                speed = lane.find(f\".//{{{namespace['ns1']}}}Speed\").text if lane.find(f\".//{{{namespace['ns1']}}}Speed\") is not None else ''\n","                occupancy = lane.find(f\".//{{{namespace['ns1']}}}Occupancy\").text if lane.find(f\".//{{{namespace['ns1']}}}Occupancy\") is not None else ''\n","\n","                lane_key = f'L{lane_id}'\n","                if lane_key not in data_dict[vdid]:\n","                    data_dict[vdid][lane_key] = {}\n","\n","                data_dict[vdid][lane_key].update({\n","                    'Speed': speed,\n","                    'Occupancy': occupancy,\n","                })\n","\n","                for vehicle in lane.findall(f\".//{{{namespace['ns1']}}}Vehicle\"):\n","                    vehicle_type = vehicle.find(f\".//{{{namespace['ns1']}}}VehicleType\").text if vehicle.find(f\".//{{{namespace['ns1']}}}VehicleType\") is not None else ''\n","                    volume = vehicle.find(f\".//{{{namespace['ns1']}}}Volume\").text if vehicle.find(f\".//{{{namespace['ns1']}}}Volume\") is not None else ''\n","                    speed2 = vehicle.find(f\".//{{{namespace['ns1']}}}Speed\").text if vehicle.find(f\".//{{{namespace['ns1']}}}Speed\") is not None else ''\n","\n","                    data_dict[vdid][lane_key].update({\n","                        f'{vehicle_type}_Volume': volume,\n","                        f'{vehicle_type}_Vehicle_Speed': speed2,\n","                    })\n","\n","            # Clear the element to free memory\n","            elem.clear()\n","\n","    # Flattening the data structure for CSV conversion\n","    flattened_data = []\n","    for vdid, lanes in data_dict.items():\n","        row = {'VDID': vdid}\n","        for lane_id, details in lanes.items():\n","            for key, value in details.items():\n","                row[f'{lane_id}_{key}'] = value\n","        flattened_data.append(row)\n","\n","    return flattened_data\n","\n","\n","def process_file(file_name, input_dir, output_dir, namespace):\n","    try:\n","        # Construct full paths for input and output files\n","        file_path = os.path.join(input_dir, file_name)\n","        output_file = os.path.join(output_dir, file_name.replace('.xml', '.csv'))\n","\n","        # Check if corresponding CSV file already exists\n","        if os.path.exists(output_file):\n","            print(f\"Skipping {file_name} as CSV already exists.\")\n","            return\n","\n","        # Convert XML to DataFrame\n","        flattened_data = parse_xml_file(file_path, namespace)\n","        df = pd.DataFrame(flattened_data)\n","\n","        # Save to CSV, skipping index\n","        df.to_csv(output_file, index=False)\n","        return file_name\n","    except Exception as e:\n","        print(f\"Error converting file {file_name}: {e}\")\n","        return None\n","\n","def convert_xml_to_csv(directory_path, date):\n","    # Implementation remains mostly the same as before\n","    # The function now defaults to using 16 worker threads\n","\n","    input_dir = os.path.join(directory_path, date, \"decompressed\")\n","    output_dir = os.path.join(directory_path, date, \"csv\")\n","\n","    # Ensure the output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Define your XML namespace\n","    namespace = {'ns1': 'http://traffic.transportdata.tw/standard/traffic/schema/'}\n","\n","    # List all XML files in the input directory\n","    xml_files = [f for f in os.listdir(input_dir) if f.endswith('.xml')]\n","    total_files = len(xml_files)\n","\n","    # Use ThreadPoolExecutor with a specified number of workers to process files concurrently\n","    with tqdm(total=total_files) as progress:\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","            # Prepare futures for all files\n","            futures = [executor.submit(process_file, file_name, input_dir, output_dir, namespace) for file_name in xml_files]\n","\n","            # Process futures as they complete\n","            for future in concurrent.futures.as_completed(futures):\n","                result = future.result()\n","                if result:\n","                    progress.update(1)\n","\n","\n","\n","#############################################################################################################\n","\n","def process_csv_files(directory_path, date):\n","    # Define input and output directories based on the provided date\n","    input_directory = os.path.join(directory_path, date, \"csv\")\n","    output_directory = os.path.join(directory_path, date, \"VDID\")\n","\n","    # Create the output directory if it doesn't exist\n","    if not os.path.exists(output_directory):\n","        os.makedirs(output_directory)\n","\n","    # Initialize an empty list to store DataFrames\n","    dfs = []\n","\n","    # List all CSV files in the input directory\n","    csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n","    \n","    print(f\"Processing {len(csv_files)} CSV files:\")\n","    \n","    with tqdm(total=len(csv_files), unit='file') as pbar_files:\n","        for filename in csv_files:\n","            try:\n","                # Read the CSV file and insert the 'file_name' column at the beginning\n","                df = pd.read_csv(os.path.join(input_directory, filename))\n","                df.insert(0, 'file_name', filename)\n","                \n","                # Append the DataFrame to the list\n","                dfs.append(df)\n","                \n","                pbar_files.update(1)\n","            except Exception as e:\n","                # Print an error message and continue processing other files\n","                print(f'Error processing file {filename}: {e}')\n","\n","    # Concatenate all DataFrames in the list to create the combined DataFrame\n","    combined_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Clear the list of individual DataFrames to release memory\n","    dfs.clear()\n","    gc.collect()  # Manually trigger garbage collection\n","\n","    # Group the combined DataFrame by 'VDID'\n","    groups = combined_df.groupby('VDID')\n","    \n","    print(f\"\\nSaving {len(groups)} VDID-specific CSV files:\")\n","    \n","    with tqdm(total=len(groups), unit='VDID') as pbar_vdids:\n","        for vdid, group_df in groups:\n","            try:\n","                # Save the group-specific data to a CSV file in the output directory\n","                group_df.to_csv(os.path.join(output_directory, f'{vdid}.csv'), index=False)\n","                \n","                pbar_vdids.update(1)\n","            except Exception as e:\n","                # Print an error message if saving fails\n","                print(f'Error saving VDID {vdid}: {e}')\n","    \n","    print(f\"\\n{len(groups)} VDID-specific CSV files saved.\")\n","\n","    # Clear variables holding large data and manually collect garbage again\n","    del combined_df, groups\n","    gc.collect()\n","\n","\n","\n","def delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv):\n","    # Define the directory paths based on the input date\n","    compressed_directory = os.path.join(directory_path, date, 'compressed')\n","    decompressed_directory = os.path.join(directory_path, date, 'decompressed')\n","    csv_directory = os.path.join(directory_path, date, 'csv')\n","    \n","    # Helper function to delete files in a directory\n","    def delete_files_in_directory(directory):\n","        if os.path.exists(directory):\n","            file_list = os.listdir(directory)\n","            for file in file_list:\n","                file_path = os.path.join(directory, file)\n","                try:\n","                    if os.path.isfile(file_path):\n","                        os.remove(file_path)\n","                except Exception as e:\n","                    print(f\"Error deleting file: {file_path} ({e})\")\n","        print(f\"Deleted file: {directory}\")\n","    \n","    # Delete files in the specified directories based on the parameter values\n","    if delete_compressed == 1:\n","        delete_files_in_directory(compressed_directory)\n","    \n","    if delete_decompressed == 1:\n","        delete_files_in_directory(decompressed_directory)\n","    \n","    if delete_csv == 1:\n","        delete_files_in_directory(csv_directory)\n","\n","def zip_output(directory_path, date, delete_files_sp_zip=0):\n","    try:\n","        # Construct the path to the directory to zip\n","        dir_to_zip = os.path.join(directory_path, date)\n","        \n","        # Check if the directory exists\n","        if not os.path.exists(dir_to_zip):\n","            print(f\"Directory {dir_to_zip} does not exist.\")\n","            return\n","        \n","        # Output zip file path\n","        output_zip_path = f\"{dir_to_zip}.zip\"\n","        \n","        # Name of the root folder within the zip file\n","        root_folder_name = os.path.basename(dir_to_zip)\n","        \n","        # Gather all files to zip\n","        files_to_zip = []\n","        for root, dirs, files in os.walk(dir_to_zip):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.join(root_folder_name, os.path.relpath(file_path, dir_to_zip))\n","                files_to_zip.append((file_path, arcname))\n","        \n","        file_count = len(files_to_zip)\n","        \n","        # Notify user about the zipping process\n","        print(f\"Zipping {file_count} files in {dir_to_zip}, please wait...\")\n","        \n","        with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            # Wrap the files_to_zip list with tqdm for a progress bar\n","            for file_path, arcname in tqdm(files_to_zip, desc=\"Zipping\"):\n","                zipf.write(file_path, arcname)\n","        \n","        # Notify completion\n","        print(f\"Completed zipping directory {dir_to_zip} into {output_zip_path}\")\n","        \n","        # Delete the original directory if delete_files_sp_zip equals 1\n","        if delete_files_sp_zip == 1:\n","            shutil.rmtree(dir_to_zip)\n","            print(f\"Deleted directory {dir_to_zip}\")\n","        \n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","\n","\n","def get_yesterday_date(timezone):\n","    # Convert the current time to the specified timezone\n","    tz = pytz.timezone(timezone)\n","    now_in_timezone = datetime.now(tz)\n","    \n","    # Calculate yesterday's date\n","    yesterday_in_timezone = now_in_timezone - timedelta(days=1)\n","    \n","    # Format yesterday's date as \"YYYYMMDD\"\n","    return yesterday_in_timezone.strftime(\"%Y%m%d\")\n","\n","# Set timezone to Taipei\n","timezone = \"Asia/Taipei\"\n","yesterday_date = get_yesterday_date(timezone)\n","print(f\"yesterday date: {yesterday_date}\")\n","\n","\n","# Main Program\n","\n","def fetch_vd(directory_path, date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    download_files_for_day(directory_path, date, max_concurrent_downloads=10)\n","    decompress_files(directory_path, date)\n","    convert_xml_to_csv(directory_path, date)\n","    process_csv_files(directory_path, date)\n","    delete_files(directory_path, date, delete_compressed, delete_decompressed, delete_csv)\n","    zip_output(directory_path, date, delete_files_sp_zip)\n","    \n","    \n","def batch_fetch_vd(start_date, num_days_backwards, directory_path, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip):\n","    # Convert start_date string to datetime object\n","    date_format = \"%Y%m%d\"\n","    current_date = datetime.strptime(start_date, date_format)\n","    \n","    # Iterate backwards from start_date for num_days_backwards\n","    for _ in range(num_days_backwards):\n","        # Convert current_date back to string and call fetch_vd\n","        formatted_date = current_date.strftime(date_format)\n","        fetch_vd(directory_path, formatted_date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip)\n","        \n","        # Decrement the day by one\n","        current_date -= timedelta(days=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:05:30.205994Z","iopub.status.busy":"2024-03-02T17:05:30.205577Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting download for date: 20230531\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4060067865024d24b74bcd76dc5404f4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a0373a0dee74a9c9e79b88666daa28b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bb3863662d5484e89c0144b80dcee02","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d94f2711110240c887f054acbff7e9cd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3847 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c584b728b6c346539e5da02e48dbc918","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3847 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3847 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230531\\compressed\n","Deleted file: D:\\VD_data\\20230531\\decompressed\n","Zipping 5287 files in D:\\VD_data\\20230531, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef0e167ba4af4333b5cce1b64bf4ffb2","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5287 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230531 into D:\\VD_data\\20230531.zip\n","Deleted directory D:\\VD_data\\20230531\n","Starting download for date: 20230530\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e3c6ed2948e43619f76c928c3b2de08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c1383e3171643779d8ddbe435a75ba8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b25330a7aa741c6baabc12fc976b593","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af3e27c39c8c40928972615cd309f324","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3841 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f21408a8f2af4be998062dd75afa692e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3841 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3841 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230530\\compressed\n","Deleted file: D:\\VD_data\\20230530\\decompressed\n","Zipping 5281 files in D:\\VD_data\\20230530, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4b1d7c6bd684c7a8097f437e0506f1f","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5281 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230530 into D:\\VD_data\\20230530.zip\n","Deleted directory D:\\VD_data\\20230530\n","Starting download for date: 20230529\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7d66a2545f74ac081ff631e90e7290b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bbb03060e9941a1a049708285633b30","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3fd5cbd7d144cbbb0e582aacc07f8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3051d25679e4d27bab494c756c2dac1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3843 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a97a3f57f52c40138ebb09dad6d9e947","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3843 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3843 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230529\\compressed\n","Deleted file: D:\\VD_data\\20230529\\decompressed\n","Zipping 5283 files in D:\\VD_data\\20230529, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a53bf72e1bd4e82b69ee5bb8d67e32a","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5283 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230529 into D:\\VD_data\\20230529.zip\n","Deleted directory D:\\VD_data\\20230529\n","Starting download for date: 20230528\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4881210ff71a491cb42dcb8c122a02b6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f34e668c5fb646f3a86a7113b78a9ef9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9de0fb9c270d45c0948f1e5e937e8c95","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae09e67004d74baeb1a57e91c338381c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3844 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7e0501f383f48db84fe5abeb52ab792","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3844 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3844 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230528\\compressed\n","Deleted file: D:\\VD_data\\20230528\\decompressed\n","Zipping 5284 files in D:\\VD_data\\20230528, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1f672258a964adbb09051acc141a0ee","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5284 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230528 into D:\\VD_data\\20230528.zip\n","Deleted directory D:\\VD_data\\20230528\n","Starting download for date: 20230527\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcf1f0e6707a4e0a93fc2d65d50e5b30","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"356e257416eb4f9b9044948653313ba8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19aa5c8fc9c34d56862426ca49887a4a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a0573bb169442dc80e214439de6f0c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3844 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d272f971b034c3e9dbd16076b0de1dc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3844 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3844 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230527\\compressed\n","Deleted file: D:\\VD_data\\20230527\\decompressed\n","Zipping 5284 files in D:\\VD_data\\20230527, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"636f9ad51b794aef8ee4c98837aeacdb","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5284 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230527 into D:\\VD_data\\20230527.zip\n","Deleted directory D:\\VD_data\\20230527\n","Starting download for date: 20230526\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"681f71517c32467792e2c392a5c7edad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d12dbcc2b36a4be587649818edd43e33","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"585eca5104be4e12aa9471fd8af47c65","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdb0de60432c4892bd14bdef8fbf784b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3845 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ba639e2ed674dad8bb41004dabad02b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3845 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3845 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230526\\compressed\n","Deleted file: D:\\VD_data\\20230526\\decompressed\n","Zipping 5285 files in D:\\VD_data\\20230526, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dd7f6272f98469e9b7f44aea3456196","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230526 into D:\\VD_data\\20230526.zip\n","Deleted directory D:\\VD_data\\20230526\n","Starting download for date: 20230525\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02a457fa7f424279a53fc9c90b7fcca5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3fcf6be7cf34feab9c5ae24c685aef5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b44b954db8141d3943a9114b735ef39","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a8f0630130c4eb6bf48d3b73e805ddf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3845 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c09b4c72409486694f624c732526a34","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3845 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3845 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230525\\compressed\n","Deleted file: D:\\VD_data\\20230525\\decompressed\n","Zipping 5285 files in D:\\VD_data\\20230525, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2339d69611b4f25b0c9df2043d7576c","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230525 into D:\\VD_data\\20230525.zip\n","Deleted directory D:\\VD_data\\20230525\n","Starting download for date: 20230524\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f3365a56f7d4c50876246aec52ea5b9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8dbc4c78f3ef442587086589045bb6a0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bb90c39952c40cbadc232dc64474cd7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing 1440 CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"617ac96e3a6d46febc8f5a9af6fb0755","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?file/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Saving 3845 VDID-specific CSV files:\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccdb08b982604bc3b0e7bd0cd7bebb5a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3845 [00:00<?, ?VDID/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","3845 VDID-specific CSV files saved.\n","Deleted file: D:\\VD_data\\20230524\\compressed\n","Deleted file: D:\\VD_data\\20230524\\decompressed\n","Zipping 5285 files in D:\\VD_data\\20230524, please wait...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce163448f00e44dca63ee7021e67180d","version_major":2,"version_minor":0},"text/plain":["Zipping:   0%|          | 0/5285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Completed zipping directory D:\\VD_data\\20230524 into D:\\VD_data\\20230524.zip\n","Deleted directory D:\\VD_data\\20230524\n","Starting download for date: 20230523\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72494d1edb6345439175fad06d0afdd1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Download process completed.\n","Decompressing xml.gz files...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fb107c54752493b95dbb998f175910f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decompression process completed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1ba73832ca24e2fa0b6876ce6281932","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#fetch_vd(directory_path, \"20240228\", 1, 1, 0, 1) # date, delete_compressed, delete_decompressed, delete_csv, delete_files_sp_zip\n","\n","#fetch_vd(directory_path, yesterday_date, 1, 1, 0, 0)\n","\n","# Example usage\n","# You need to replace \"your_directory_path_here\" with the actual directory path.\n","# Also, adjust the boolean flags as needed for your use case.\n","batch_fetch_vd(\"20230531\", 31, directory_path, delete_compressed=True, delete_decompressed=True, delete_csv=False, delete_files_sp_zip=True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
